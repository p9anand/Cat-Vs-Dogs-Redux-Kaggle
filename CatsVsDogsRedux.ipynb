{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Action Plan\n",
    "\n",
    "1) load vgg model and classes from keras\n",
    "\n",
    "2) define paths to load the data\n",
    "\n",
    "3) load and dump data using bcolz\n",
    "\n",
    "4) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from keras.applications import vgg16\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "import numpy as np\n",
    "from keras.utils.data_utils import get_file\n",
    "import zipfile\n",
    "import json\n",
    "from keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import os, json\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "from scipy import misc, ndimage\n",
    "from scipy.ndimage.interpolation import zoom\n",
    "\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras import backend as K\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Flatten, Dense, Dropout, Lambda\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D, ZeroPadding2D\n",
    "from keras.layers.pooling import GlobalAveragePooling2D\n",
    "from keras.optimizers import SGD, RMSprop, Adam\n",
    "from keras.preprocessing import image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Function to download data\n",
    "from __future__ import ( division, absolute_import, print_function, unicode_literals )\n",
    "\n",
    "import sys, os, tempfile, logging\n",
    "\n",
    "if sys.version_info >= (3,):\n",
    "    import urllib.request as urllib2\n",
    "    import urllib.parse as urlparse\n",
    "else:\n",
    "    import urllib2\n",
    "    import urlparse\n",
    "\n",
    "def download_file(url, desc=None):\n",
    "    u = urllib2.urlopen(url)\n",
    "\n",
    "    scheme, netloc, path, query, fragment = urlparse.urlsplit(url)\n",
    "    filename = os.path.basename(path)\n",
    "    if not filename:\n",
    "        filename = 'downloaded.file'\n",
    "    if desc:\n",
    "        filename = os.path.join(desc, filename)\n",
    "\n",
    "    with open(filename, 'wb') as f:\n",
    "        meta = u.info()\n",
    "        meta_func = meta.getheaders if hasattr(meta, 'getheaders') else meta.get_all\n",
    "        meta_length = meta_func(\"Content-Length\")\n",
    "        file_size = None\n",
    "        if meta_length:\n",
    "            file_size = int(meta_length[0])\n",
    "        print(\"Downloading: {0} Bytes: {1}\".format(url, file_size))\n",
    "\n",
    "        file_size_dl = 0\n",
    "        block_sz = 8192\n",
    "        while True:\n",
    "            buffer = u.read(block_sz)\n",
    "            if not buffer:\n",
    "                break\n",
    "\n",
    "            file_size_dl += len(buffer)\n",
    "            f.write(buffer)\n",
    "\n",
    "            status = \"{0:16}\".format(file_size_dl)\n",
    "            if file_size:\n",
    "                status += \"   [{0:6.2f}%]\".format(file_size_dl * 100 / file_size)\n",
    "            status += chr(13)\n",
    "            print(status, end=\"\")\n",
    "        print()\n",
    "    with zipfile.ZipFile(\"dogscats.zip\",\"r\") as zip_ref:\n",
    "        zip_ref.extractall()\n",
    "\n",
    "    return filename\n",
    "\n",
    "url = files_to_download = \"http://www.platform.ai/data/dogscats.zip\"\n",
    "# filename = download_file(url)\n",
    "# print(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "CLASS_FILE='/home/pranand/.keras/models/imagenet_class_index.json'\n",
    "def model_load(model=vgg16.VGG16(), class_path=CLASS_FILE, ):\n",
    "    model = model\n",
    "    classes_dict = json.load(open(class_path))\n",
    "    classes = [classes_dict[str(i)][1] for i in range(len(classes_dict))]\n",
    "    return model, classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model, classes = model_load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "## Defining  a function to get data in batches from a directory\n",
    "# path = '/home/pranand/courses/deeplearning1/nbs/My_Files'\n",
    "path = '/home/pranand/courses/deeplearning1/nbs/My_Files/dogscats/'\n",
    "\n",
    "batch_size=4\n",
    "def get_batches(directory, gen=image.ImageDataGenerator(), shuffle=True, batch_size=batch_size, \n",
    "                class_mode='categorical', target_size=None):\n",
    "    if target_size == None:\n",
    "        target_size = (224, 224)\n",
    "    return gen.flow_from_directory(directory, target_size=target_size, class_mode=class_mode, batch_size=batch_size\n",
    "                                  , shuffle=shuffle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "## Append data in an array using get_data function\n",
    "def get_data(batches):\n",
    "    return np.concatenate([batches.next() for i in range(batches.nb_sample)])\n",
    "\n",
    "# def onehot(x): return np.array(OneHotEncoder().fit_transform(x.reshape(-1,1)).todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "## Loading and saving an array in python\n",
    "\n",
    "import bcolz\n",
    "def save_array(fname, arr): c=bcolz.carray(arr, rootdir=fname, mode='w'); c.flush()\n",
    "def load_array(fname): return bcolz.open(fname)[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 23000 images belonging to 2 classes.\n",
      "Found 2000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "batches = get_batches(path + 'train', batch_size=batch_size)\n",
    "val_batches = get_batches(path + 'valid', batch_size=batch_size)\n",
    "# imgs_train,labels_train = next(batches)\n",
    "# imgs_val,labels_val = next(val_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "## Saving the array on disk\n",
    "# train_data = get_data(batches)\n",
    "# val_data = get_data(val_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def predict_using_vgg(batches):\n",
    "    train_feature = model.predict_generator(batches, batches.nb_sample)\n",
    "    return train_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "trn_features = predict_using_vgg(batches=batches)\n",
    "val_features = predict_using_vgg(batches=val_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print(trn_features.shape)\n",
    "print(val_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Saving using bcolz\n",
    "\n",
    "save_array('train_data.bc', trn_features)\n",
    "save_array('valid_data.bc', val_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# loading dumped data\n",
    "train_data = load_array('train_data.bc')\n",
    "val_data = load_array('valid_data.bc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "def onehot(x): return np.array(OneHotEncoder().fit_transform(x.reshape(-1,1)).todense())\n",
    "\n",
    "val_classes = val_batches.classes\n",
    "trn_classes = batches.classes\n",
    "val_labels = onehot(val_classes)\n",
    "trn_labels = onehot(trn_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def newmodel(batches, lr=0.01):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(input_dim=1000, output_dim=batches.nb_class, activation='softmax'))\n",
    "    model.compile(optimizer=Adam(lr=lr),\n",
    "                loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batches.nb_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "Newmodel = newmodel(batches)\n",
    "# lm.fit(trn_features, trn_labels, nb_epoch=3, batch_size=batch_size, \n",
    "#        validation_data=(val_features, val_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "no_of_epochs = 1\n",
    "for epoch in range(no_of_epochs):\n",
    "    print(\"Running epoch: %d\" % epoch)\n",
    "    Newmodel.fit(train_data, trn_labels, nb_epoch=1, validation_data=(val_data, val_labels))\n",
    "    latest_weights_filename = 'ft%d.h5' % epoch\n",
    "    Newmodel.model.save_weights(path+latest_weights_filename)\n",
    "print(\"Completed %s fit operations\" % no_of_epochs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
